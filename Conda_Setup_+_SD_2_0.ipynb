{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "authorship_tag": "ABX9TyOckEiEdIIObdhCZqVxgFeC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meefs/art/blob/master/Conda_Setup_%2B_SD_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🐍 miniconda setup and installation"
      ],
      "metadata": {
        "id": "OfIGQg3F05Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup miniconda environment"
      ],
      "metadata": {
        "id": "cSJE13Kiz9TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "B3VUO9vEgQXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed61459-a174-4a33-cc74-65fed3992970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 24 17:44:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which version of python? Where is it located at?"
      ],
      "metadata": {
        "id": "bMGtCerMxarm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP25PD4xadx5",
        "outputId": "3a9aabc0-ee8f-471a-da0c-1663ca4a94f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.15\n",
            "/usr/local/bin/python\n",
            "/env/python\n",
            "env: PYTHONPATH=\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python\n",
        "!echo $PYTHONPATH\n",
        "%env PYTHONPATH=\n",
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and run miniconda"
      ],
      "metadata": {
        "id": "rOzxLtV6xqFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b2eCsdOd0u6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxvNAVQult4h",
        "outputId": "a557f38a-69b6-40b6-9174-b0ba7a2251ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-24 17:44:50--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76607678 (73M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>]  73.06M   127MB/s    in 0.6s    \n",
            "\n",
            "2022-11-24 17:44:51 (127 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [76607678/76607678]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - _openmp_mutex==4.5=1_gnu\n",
            "    - brotlipy==0.7.0=py39h27cfd23_1003\n",
            "    - ca-certificates==2022.3.29=h06a4308_1\n",
            "    - certifi==2021.10.8=py39h06a4308_2\n",
            "    - cffi==1.15.0=py39hd667e15_1\n",
            "    - charset-normalizer==2.0.4=pyhd3eb1b0_0\n",
            "    - colorama==0.4.4=pyhd3eb1b0_0\n",
            "    - conda-content-trust==0.1.1=pyhd3eb1b0_0\n",
            "    - conda-package-handling==1.8.1=py39h7f8727e_0\n",
            "    - conda==4.12.0=py39h06a4308_0\n",
            "    - cryptography==36.0.0=py39h9ce1e76_0\n",
            "    - idna==3.3=pyhd3eb1b0_0\n",
            "    - ld_impl_linux-64==2.35.1=h7274673_9\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.3.0=h5101ec6_17\n",
            "    - libgomp==9.3.0=h5101ec6_17\n",
            "    - libstdcxx-ng==9.3.0=hd4cf53a_17\n",
            "    - ncurses==6.3=h7f8727e_2\n",
            "    - openssl==1.1.1n=h7f8727e_0\n",
            "    - pip==21.2.4=py39h06a4308_0\n",
            "    - pycosat==0.6.3=py39h27cfd23_0\n",
            "    - pycparser==2.21=pyhd3eb1b0_0\n",
            "    - pyopenssl==22.0.0=pyhd3eb1b0_0\n",
            "    - pysocks==1.7.1=py39h06a4308_0\n",
            "    - python==3.9.12=h12debd9_0\n",
            "    - readline==8.1.2=h7f8727e_1\n",
            "    - requests==2.27.1=pyhd3eb1b0_0\n",
            "    - ruamel_yaml==0.15.100=py39h27cfd23_0\n",
            "    - setuptools==61.2.0=py39h06a4308_0\n",
            "    - six==1.16.0=pyhd3eb1b0_1\n",
            "    - sqlite==3.38.2=hc218d9a_0\n",
            "    - tk==8.6.11=h1ccaba5_0\n",
            "    - tqdm==4.63.0=pyhd3eb1b0_0\n",
            "    - tzdata==2022a=hda174b7_0\n",
            "    - urllib3==1.26.8=pyhd3eb1b0_0\n",
            "    - wheel==0.37.1=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.12=h7f8727e_1\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py39h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.3.29-h06a4308_1\n",
            "  certifi            pkgs/main/linux-64::certifi-2021.10.8-py39h06a4308_2\n",
            "  cffi               pkgs/main/linux-64::cffi-1.15.0-py39hd667e15_1\n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
            "  colorama           pkgs/main/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  conda              pkgs/main/linux-64::conda-4.12.0-py39h06a4308_0\n",
            "  conda-content-tru~ pkgs/main/noarch::conda-content-trust-0.1.1-pyhd3eb1b0_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.8.1-py39h7f8727e_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-36.0.0-py39h9ce1e76_0\n",
            "  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.35.1-h7274673_9\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.3.0-h5101ec6_17\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.3.0-hd4cf53a_17\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1n-h7f8727e_0\n",
            "  pip                pkgs/main/linux-64::pip-21.2.4-py39h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py39h27cfd23_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-pyhd3eb1b0_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py39h06a4308_0\n",
            "  python             pkgs/main/linux-64::python-3.9.12-h12debd9_0\n",
            "  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1\n",
            "  requests           pkgs/main/noarch::requests-2.27.1-pyhd3eb1b0_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.100-py39h27cfd23_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-61.2.0-py39h06a4308_0\n",
            "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.38.2-hc218d9a_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.11-h1ccaba5_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.63.0-pyhd3eb1b0_0\n",
            "  tzdata             pkgs/main/noarch::tzdata-2022a-hda174b7_0\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.8-pyhd3eb1b0_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.12-h7f8727e_1\n",
            "\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which version of conda? Where is it at?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Which version of python? Where is it at??\n"
      ],
      "metadata": {
        "id": "2nuBRppRxx_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version\n",
        "!which conda\n",
        "\n",
        "!python --version\n",
        "!which python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVM0ksV0mIOf",
        "outputId": "41ba896c-6e0b-42da-afda-911a695a9853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda 4.12.0\n",
            "/usr/local/bin/conda\n",
            "Python 3.9.12\n",
            "/usr/local/bin/python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify sys.path for python. Append directory /usr/local/lib/python3.9/site-packages"
      ],
      "metadata": {
        "id": "DJNG4fbUyI_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UllLc-NpAIy",
        "outputId": "3de8e5f4-784f-45cc-8b3d-f12543048436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_= (sys.path.append(\"/usr/local/lib/python3.9/site-packages\"))"
      ],
      "metadata": {
        "id": "cknMt4wPpCN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Xl9M4YqTks",
        "outputId": "88792071-c1d5-4d89-aa39-0e16b4a224b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " '/usr/local/lib/python3.9/site-packages']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update Conda 🐍"
      ],
      "metadata": {
        "id": "eddorMlOyazL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install --channel defaults conda python=3.9 --yes\n",
        "!conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL6kmI3iqXxn",
        "outputId": "ea6bf843-f6b6-44f8-c41b-adc5ea8d1f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda\n",
            "    - python=3.9\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2022.10.11 |       h06a4308_0         124 KB\n",
            "    certifi-2022.9.24          |   py39h06a4308_0         154 KB\n",
            "    conda-22.9.0               |   py39h06a4308_0         884 KB\n",
            "    openssl-1.1.1s             |       h7f8727e_0         3.6 MB\n",
            "    toolz-0.12.0               |   py39h06a4308_0         105 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         4.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  toolz              pkgs/main/linux-64::toolz-0.12.0-py39h06a4308_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                      2022.3.29-h06a4308_1 --> 2022.10.11-h06a4308_0\n",
            "  certifi                          2021.10.8-py39h06a4308_2 --> 2022.9.24-py39h06a4308_0\n",
            "  conda                               4.12.0-py39h06a4308_0 --> 22.9.0-py39h06a4308_0\n",
            "  openssl                                 1.1.1n-h7f8727e_0 --> 1.1.1s-h7f8727e_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "conda-22.9.0         | 884 KB    | : 100% 1.0/1 [00:00<00:00,  5.41it/s]               \n",
            "openssl-1.1.1s       | 3.6 MB    | : 100% 1.0/1 [00:00<00:00,  7.81it/s]\n",
            "ca-certificates-2022 | 124 KB    | : 100% 1.0/1 [00:00<00:00, 19.35it/s]\n",
            "certifi-2022.9.24    | 154 KB    | : 100% 1.0/1 [00:00<00:00, 19.86it/s]\n",
            "toolz-0.12.0         | 105 KB    | : 100% 1.0/1 [00:00<00:00, 17.89it/s]\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\bdone\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-5.1          |            1_gnu          21 KB\n",
            "    cffi-1.15.1                |   py39h74dc2b5_0         228 KB\n",
            "    conda-package-handling-1.9.0|   py39h5eee18b_1         946 KB\n",
            "    cryptography-38.0.1        |   py39h9ce1e76_0         1.3 MB\n",
            "    idna-3.4                   |   py39h06a4308_0          93 KB\n",
            "    ld_impl_linux-64-2.38      |       h1181459_1         654 KB\n",
            "    libffi-3.4.2               |       h6a678d5_6         136 KB\n",
            "    libgcc-ng-11.2.0           |       h1234567_1         5.3 MB\n",
            "    libgomp-11.2.0             |       h1234567_1         474 KB\n",
            "    libstdcxx-ng-11.2.0        |       h1234567_1         4.7 MB\n",
            "    ncurses-6.3                |       h5eee18b_3         781 KB\n",
            "    pip-22.2.2                 |   py39h06a4308_0         2.3 MB\n",
            "    pycosat-0.6.4              |   py39h5eee18b_0          84 KB\n",
            "    python-3.9.15              |       h7a1cb2a_2        25.0 MB\n",
            "    readline-8.2               |       h5eee18b_0         357 KB\n",
            "    requests-2.28.1            |   py39h06a4308_0          92 KB\n",
            "    setuptools-65.5.0          |   py39h06a4308_0         1.1 MB\n",
            "    sqlite-3.40.0              |       h5082296_0         1.2 MB\n",
            "    tk-8.6.12                  |       h1ccaba5_0         3.0 MB\n",
            "    tqdm-4.64.1                |   py39h06a4308_0         125 KB\n",
            "    tzdata-2022f               |       h04d1e81_0         115 KB\n",
            "    urllib3-1.26.12            |   py39h06a4308_0         181 KB\n",
            "    xz-5.2.6                   |       h5eee18b_0         394 KB\n",
            "    zlib-1.2.13                |       h5eee18b_0         103 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        48.7 MB\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  colorama-0.4.4-pyhd3eb1b0_0\n",
            "  conda-content-trust-0.1.1-pyhd3eb1b0_0\n",
            "  six-1.16.0-pyhd3eb1b0_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-1_gnu --> 5.1-1_gnu None\n",
            "  cffi                                1.15.0-py39hd667e15_1 --> 1.15.1-py39h74dc2b5_0 None\n",
            "  conda-package-han~                   1.8.1-py39h7f8727e_0 --> 1.9.0-py39h5eee18b_1 None\n",
            "  cryptography                        36.0.0-py39h9ce1e76_0 --> 38.0.1-py39h9ce1e76_0 None\n",
            "  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0~ --> pkgs/main/linux-64::idna-3.4-py39h06a4308_0 None\n",
            "  ld_impl_linux-64                        2.35.1-h7274673_9 --> 2.38-h1181459_1 None\n",
            "  libffi                                     3.3-he6710b0_2 --> 3.4.2-h6a678d5_6 None\n",
            "  libgcc-ng                               9.3.0-h5101ec6_17 --> 11.2.0-h1234567_1 None\n",
            "  libgomp                                 9.3.0-h5101ec6_17 --> 11.2.0-h1234567_1 None\n",
            "  libstdcxx-ng                            9.3.0-hd4cf53a_17 --> 11.2.0-h1234567_1 None\n",
            "  ncurses                                    6.3-h7f8727e_2 --> 6.3-h5eee18b_3 None\n",
            "  pip                                 21.2.4-py39h06a4308_0 --> 22.2.2-py39h06a4308_0 None\n",
            "  pycosat                              0.6.3-py39h27cfd23_0 --> 0.6.4-py39h5eee18b_0 None\n",
            "  python                                  3.9.12-h12debd9_0 --> 3.9.15-h7a1cb2a_2 None\n",
            "  readline                                 8.1.2-h7f8727e_1 --> 8.2-h5eee18b_0 None\n",
            "  requests           pkgs/main/noarch::requests-2.27.1-pyh~ --> pkgs/main/linux-64::requests-2.28.1-py39h06a4308_0 None\n",
            "  setuptools                          61.2.0-py39h06a4308_0 --> 65.5.0-py39h06a4308_0 None\n",
            "  sqlite                                  3.38.2-hc218d9a_0 --> 3.40.0-h5082296_0 None\n",
            "  tk                                      8.6.11-h1ccaba5_0 --> 8.6.12-h1ccaba5_0 None\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.63.0-pyhd3eb~ --> pkgs/main/linux-64::tqdm-4.64.1-py39h06a4308_0 None\n",
            "  tzdata                                   2022a-hda174b7_0 --> 2022f-h04d1e81_0 None\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.8-pyhd~ --> pkgs/main/linux-64::urllib3-1.26.12-py39h06a4308_0 None\n",
            "  xz                                       5.2.5-h7b6447c_0 --> 5.2.6-h5eee18b_0 None\n",
            "  zlib                                    1.2.12-h7f8727e_1 --> 1.2.13-h5eee18b_0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "tk-8.6.12            | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  4.23it/s]\n",
            "libffi-3.4.2         | 136 KB    | : 100% 1.0/1 [00:00<00:00, 14.37it/s]\n",
            "zlib-1.2.13          | 103 KB    | : 100% 1.0/1 [00:00<00:00, 19.39it/s]\n",
            "pip-22.2.2           | 2.3 MB    | : 100% 1.0/1 [00:00<00:00,  5.41it/s]\n",
            "tzdata-2022f         | 115 KB    | : 100% 1.0/1 [00:00<00:00, 11.30it/s]\n",
            "urllib3-1.26.12      | 181 KB    | : 100% 1.0/1 [00:00<00:00, 17.56it/s]\n",
            "libgcc-ng-11.2.0     | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  5.78it/s]\n",
            "libstdcxx-ng-11.2.0  | 4.7 MB    | : 100% 1.0/1 [00:00<00:00,  6.01it/s]\n",
            "python-3.9.15        | 25.0 MB   | : 100% 1.0/1 [00:00<00:00,  1.54it/s]               \n",
            "_openmp_mutex-5.1    | 21 KB     | : 100% 1.0/1 [00:00<00:00, 14.30it/s]\n",
            "readline-8.2         | 357 KB    | : 100% 1.0/1 [00:00<00:00, 17.74it/s]\n",
            "setuptools-65.5.0    | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  8.68it/s]\n",
            "ld_impl_linux-64-2.3 | 654 KB    | : 100% 1.0/1 [00:00<00:00, 13.90it/s]\n",
            "cryptography-38.0.1  | 1.3 MB    | : 100% 1.0/1 [00:00<00:00, 10.01it/s]\n",
            "xz-5.2.6             | 394 KB    | : 100% 1.0/1 [00:00<00:00, 14.48it/s]\n",
            "cffi-1.15.1          | 228 KB    | : 100% 1.0/1 [00:00<00:00, 17.02it/s]\n",
            "pycosat-0.6.4        | 84 KB     | : 100% 1.0/1 [00:00<00:00, 18.60it/s]\n",
            "conda-package-handli | 946 KB    | : 100% 1.0/1 [00:00<00:00, 14.63it/s]\n",
            "sqlite-3.40.0        | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 14.45it/s]\n",
            "idna-3.4             | 93 KB     | : 100% 1.0/1 [00:00<00:00, 19.46it/s]\n",
            "ncurses-6.3          | 781 KB    | : 100% 1.0/1 [00:00<00:00,  4.29it/s]\n",
            "libgomp-11.2.0       | 474 KB    | : 100% 1.0/1 [00:00<00:00, 15.68it/s]\n",
            "requests-2.28.1      | 92 KB     | : 100% 1.0/1 [00:00<00:00, 15.39it/s]\n",
            "tqdm-4.64.1          | 125 KB    | : 100% 1.0/1 [00:00<00:00, 16.42it/s]\n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Retrieving notices: ...working... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install featuretools https://anaconda.org/conda-forge/featuretools"
      ],
      "metadata": {
        "id": "YVpNA86Ky2zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install --channel conda-forge featuretools --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jWzipMsw0WN",
        "outputId": "c6349295-f77f-47cd-ad2a-038b987b1f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - featuretools\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    bokeh-2.4.3                |     pyhd8ed1ab_3        13.3 MB  conda-forge\n",
            "    bottleneck-1.3.5           |   py39h7deecbd_0         115 KB\n",
            "    ca-certificates-2022.9.24  |       ha878542_0         150 KB  conda-forge\n",
            "    certifi-2022.9.24          |     pyhd8ed1ab_0         155 KB  conda-forge\n",
            "    click-8.1.3                |unix_pyhd8ed1ab_2          74 KB  conda-forge\n",
            "    cloudpickle-2.2.0          |     pyhd8ed1ab_0          25 KB  conda-forge\n",
            "    conda-22.9.0               |   py39hf3d152e_2         963 KB  conda-forge\n",
            "    convertdate-2.4.0          |     pyhd8ed1ab_0          38 KB  conda-forge\n",
            "    cytoolz-0.12.0             |   py39h5eee18b_0         373 KB\n",
            "    dask-2022.11.1             |     pyhd8ed1ab_0           7 KB  conda-forge\n",
            "    dask-core-2022.11.1        |     pyhd8ed1ab_0         796 KB  conda-forge\n",
            "    distributed-2022.11.1      |     pyhd8ed1ab_0         711 KB  conda-forge\n",
            "    featuretools-1.18.0        |     pyhd8ed1ab_0         564 KB  conda-forge\n",
            "    fftw-3.3.10                |nompi_h77c792f_102         6.4 MB  conda-forge\n",
            "    freetype-2.10.4            |       h0708190_1         890 KB  conda-forge\n",
            "    fsspec-2022.11.0           |     pyhd8ed1ab_0          96 KB  conda-forge\n",
            "    giflib-5.2.1               |       h36c2ea0_2          77 KB  conda-forge\n",
            "    heapdict-1.0.1             |             py_0           7 KB  conda-forge\n",
            "    hijri-converter-2.2.4      |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    holidays-0.17.2            |     pyhd8ed1ab_0          98 KB  conda-forge\n",
            "    intel-openmp-2021.4.0      |    h06a4308_3561         4.2 MB\n",
            "    jinja2-3.1.2               |     pyhd8ed1ab_1          99 KB  conda-forge\n",
            "    joblib-1.2.0               |     pyhd8ed1ab_0         205 KB  conda-forge\n",
            "    jpeg-9e                    |       h166bdaf_1         268 KB  conda-forge\n",
            "    korean_lunar_calendar-0.3.1|     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    lcms2-2.12                 |       hddcbb42_0         443 KB  conda-forge\n",
            "    lerc-3.0                   |       h9c3ff4c_0         216 KB  conda-forge\n",
            "    libdeflate-1.8             |       h7f8727e_5          51 KB\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    libpng-1.6.37              |       hbc83047_0         278 KB\n",
            "    libtiff-4.4.0              |       hecacb30_2         526 KB\n",
            "    libwebp-1.2.4              |       h11a3e52_0          79 KB\n",
            "    libwebp-base-1.2.4         |       h5eee18b_0         347 KB\n",
            "    locket-1.0.0               |     pyhd8ed1ab_0           8 KB  conda-forge\n",
            "    lz4-3.1.3                  |   py39h27cfd23_0          42 KB\n",
            "    lz4-c-1.9.3                |       h9c3ff4c_1         179 KB  conda-forge\n",
            "    markupsafe-2.1.1           |   py39hb9d737c_1          22 KB  conda-forge\n",
            "    mkl-2021.4.0               |     h06a4308_640       142.6 MB\n",
            "    mkl-service-2.4.0          |   py39h7e14d7c_0          61 KB  conda-forge\n",
            "    mkl_fft-1.3.1              |   py39h0c7bc48_1         213 KB  conda-forge\n",
            "    mkl_random-1.2.2           |   py39hde0f152_0         379 KB  conda-forge\n",
            "    msgpack-python-1.0.3       |   py39hd09550d_0          83 KB\n",
            "    numexpr-2.8.4              |   py39he184ba9_0         133 KB\n",
            "    numpy-1.23.4               |   py39h14f4228_0          10 KB\n",
            "    numpy-base-1.23.4          |   py39h31eccc5_0         6.7 MB\n",
            "    packaging-21.3             |     pyhd8ed1ab_0          36 KB  conda-forge\n",
            "    pandas-1.5.1               |   py39h417a72b_0        11.9 MB\n",
            "    partd-1.3.0                |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    pillow-9.2.0               |   py39hace64e9_1         670 KB\n",
            "    psutil-5.9.0               |   py39h5eee18b_0         330 KB\n",
            "    pymeeus-0.5.10             |     pyhd8ed1ab_0         534 KB  conda-forge\n",
            "    pyparsing-3.0.9            |     pyhd8ed1ab_0          79 KB  conda-forge\n",
            "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
            "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
            "    pytz-2022.6                |     pyhd8ed1ab_0         235 KB  conda-forge\n",
            "    pyyaml-6.0                 |   py39hb9d737c_4         178 KB  conda-forge\n",
            "    scikit-learn-1.1.3         |   py39h6a678d5_0         7.0 MB\n",
            "    scipy-1.9.3                |   py39h14f4228_0        22.2 MB\n",
            "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
            "    sortedcontainers-2.4.0     |     pyhd8ed1ab_0          26 KB  conda-forge\n",
            "    tblib-1.7.0                |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    threadpoolctl-3.1.0        |     pyh8a188c0_0          18 KB  conda-forge\n",
            "    tornado-6.1                |   py39hb9d737c_3         649 KB  conda-forge\n",
            "    typing_extensions-4.4.0    |     pyha770c72_0          29 KB  conda-forge\n",
            "    woodwork-0.20.0            |     pyhd8ed1ab_0         206 KB  conda-forge\n",
            "    zict-2.2.0                 |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    zstd-1.5.2                 |       ha4553b6_0         488 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       228.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-mkl None\n",
            "  bokeh              conda-forge/noarch::bokeh-2.4.3-pyhd8ed1ab_3 None\n",
            "  bottleneck         pkgs/main/linux-64::bottleneck-1.3.5-py39h7deecbd_0 None\n",
            "  click              conda-forge/noarch::click-8.1.3-unix_pyhd8ed1ab_2 None\n",
            "  cloudpickle        conda-forge/noarch::cloudpickle-2.2.0-pyhd8ed1ab_0 None\n",
            "  convertdate        conda-forge/noarch::convertdate-2.4.0-pyhd8ed1ab_0 None\n",
            "  cytoolz            pkgs/main/linux-64::cytoolz-0.12.0-py39h5eee18b_0 None\n",
            "  dask               conda-forge/noarch::dask-2022.11.1-pyhd8ed1ab_0 None\n",
            "  dask-core          conda-forge/noarch::dask-core-2022.11.1-pyhd8ed1ab_0 None\n",
            "  distributed        conda-forge/noarch::distributed-2022.11.1-pyhd8ed1ab_0 None\n",
            "  featuretools       conda-forge/noarch::featuretools-1.18.0-pyhd8ed1ab_0 None\n",
            "  fftw               conda-forge/linux-64::fftw-3.3.10-nompi_h77c792f_102 None\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h0708190_1 None\n",
            "  fsspec             conda-forge/noarch::fsspec-2022.11.0-pyhd8ed1ab_0 None\n",
            "  giflib             conda-forge/linux-64::giflib-5.2.1-h36c2ea0_2 None\n",
            "  heapdict           conda-forge/noarch::heapdict-1.0.1-py_0 None\n",
            "  hijri-converter    conda-forge/noarch::hijri-converter-2.2.4-pyhd8ed1ab_0 None\n",
            "  holidays           conda-forge/noarch::holidays-0.17.2-pyhd8ed1ab_0 None\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561 None\n",
            "  jinja2             conda-forge/noarch::jinja2-3.1.2-pyhd8ed1ab_1 None\n",
            "  joblib             conda-forge/noarch::joblib-1.2.0-pyhd8ed1ab_0 None\n",
            "  jpeg               conda-forge/linux-64::jpeg-9e-h166bdaf_1 None\n",
            "  korean_lunar_cale~ conda-forge/noarch::korean_lunar_calendar-0.3.1-pyhd8ed1ab_0 None\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.12-hddcbb42_0 None\n",
            "  lerc               conda-forge/linux-64::lerc-3.0-h9c3ff4c_0 None\n",
            "  libdeflate         pkgs/main/linux-64::libdeflate-1.8-h7f8727e_5 None\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 None\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 None\n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0 None\n",
            "  libtiff            pkgs/main/linux-64::libtiff-4.4.0-hecacb30_2 None\n",
            "  libwebp            pkgs/main/linux-64::libwebp-1.2.4-h11a3e52_0 None\n",
            "  libwebp-base       pkgs/main/linux-64::libwebp-base-1.2.4-h5eee18b_0 None\n",
            "  locket             conda-forge/noarch::locket-1.0.0-pyhd8ed1ab_0 None\n",
            "  lz4                pkgs/main/linux-64::lz4-3.1.3-py39h27cfd23_0 None\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.3-h9c3ff4c_1 None\n",
            "  markupsafe         conda-forge/linux-64::markupsafe-2.1.1-py39hb9d737c_1 None\n",
            "  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640 None\n",
            "  mkl-service        conda-forge/linux-64::mkl-service-2.4.0-py39h7e14d7c_0 None\n",
            "  mkl_fft            conda-forge/linux-64::mkl_fft-1.3.1-py39h0c7bc48_1 None\n",
            "  mkl_random         conda-forge/linux-64::mkl_random-1.2.2-py39hde0f152_0 None\n",
            "  msgpack-python     pkgs/main/linux-64::msgpack-python-1.0.3-py39hd09550d_0 None\n",
            "  numexpr            pkgs/main/linux-64::numexpr-2.8.4-py39he184ba9_0 None\n",
            "  numpy              pkgs/main/linux-64::numpy-1.23.4-py39h14f4228_0 None\n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.23.4-py39h31eccc5_0 None\n",
            "  packaging          conda-forge/noarch::packaging-21.3-pyhd8ed1ab_0 None\n",
            "  pandas             pkgs/main/linux-64::pandas-1.5.1-py39h417a72b_0 None\n",
            "  partd              conda-forge/noarch::partd-1.3.0-pyhd8ed1ab_0 None\n",
            "  pillow             pkgs/main/linux-64::pillow-9.2.0-py39hace64e9_1 None\n",
            "  psutil             pkgs/main/linux-64::psutil-5.9.0-py39h5eee18b_0 None\n",
            "  pymeeus            conda-forge/noarch::pymeeus-0.5.10-pyhd8ed1ab_0 None\n",
            "  pyparsing          conda-forge/noarch::pyparsing-3.0.9-pyhd8ed1ab_0 None\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0 None\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.9-2_cp39 None\n",
            "  pytz               conda-forge/noarch::pytz-2022.6-pyhd8ed1ab_0 None\n",
            "  pyyaml             conda-forge/linux-64::pyyaml-6.0-py39hb9d737c_4 None\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-1.1.3-py39h6a678d5_0 None\n",
            "  scipy              pkgs/main/linux-64::scipy-1.9.3-py39h14f4228_0 None\n",
            "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 None\n",
            "  sortedcontainers   conda-forge/noarch::sortedcontainers-2.4.0-pyhd8ed1ab_0 None\n",
            "  tblib              conda-forge/noarch::tblib-1.7.0-pyhd8ed1ab_0 None\n",
            "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0 None\n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py39hb9d737c_3 None\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.4.0-pyha770c72_0 None\n",
            "  woodwork           conda-forge/noarch::woodwork-0.20.0-pyhd8ed1ab_0 None\n",
            "  zict               conda-forge/noarch::zict-2.2.0-pyhd8ed1ab_0 None\n",
            "  zstd               pkgs/main/linux-64::zstd-1.5.2-ha4553b6_0 None\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda              pkgs/main::conda-22.9.0-py39h06a4308_0 --> conda-forge::conda-22.9.0-py39hf3d152e_2 None\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2022.10.11~ --> conda-forge::ca-certificates-2022.9.24-ha878542_0 None\n",
            "  certifi            pkgs/main/linux-64::certifi-2022.9.24~ --> conda-forge/noarch::certifi-2022.9.24-pyhd8ed1ab_0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "hijri-converter-2.2. | 18 KB     | : 100% 1.0/1 [00:00<00:00,  9.17it/s]               \n",
            "dask-core-2022.11.1  | 796 KB    | : 100% 1.0/1 [00:00<00:00,  5.60it/s]\n",
            "blas-1.0             | 6 KB      | : 100% 1.0/1 [00:00<00:00,  7.68it/s]\n",
            "packaging-21.3       | 36 KB     | : 100% 1.0/1 [00:00<00:00, 21.64it/s]\n",
            "libgfortran5-12.2.0  | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.46it/s]               \n",
            "freetype-2.10.4      | 890 KB    | : 100% 1.0/1 [00:00<00:00,  4.40it/s]\n",
            "pymeeus-0.5.10       | 534 KB    | : 100% 1.0/1 [00:00<00:00,  6.66it/s]\n",
            "cytoolz-0.12.0       | 373 KB    | : 100% 1.0/1 [00:00<00:00,  6.97it/s]\n",
            "typing_extensions-4. | 29 KB     | : 100% 1.0/1 [00:00<00:00, 18.95it/s]\n",
            "conda-22.9.0         | 963 KB    | : 100% 1.0/1 [00:00<00:00,  3.87it/s]\n",
            "dask-2022.11.1       | 7 KB      | : 100% 1.0/1 [00:00<00:00, 11.74it/s]\n",
            "holidays-0.17.2      | 98 KB     | : 100% 1.0/1 [00:00<00:00,  8.64it/s]\n",
            "click-8.1.3          | 74 KB     | : 100% 1.0/1 [00:00<00:00, 17.66it/s]\n",
            "scipy-1.9.3          | 22.2 MB   | : 100% 1.0/1 [00:00<00:00,  1.62it/s]\n",
            "libdeflate-1.8       | 51 KB     | : 100% 1.0/1 [00:00<00:00, 13.38it/s]\n",
            "pytz-2022.6          | 235 KB    | : 100% 1.0/1 [00:00<00:00,  8.06it/s]\n",
            "intel-openmp-2021.4. | 4.2 MB    | : 100% 1.0/1 [00:00<00:00,  5.38it/s]\n",
            "convertdate-2.4.0    | 38 KB     | : 100% 1.0/1 [00:00<00:00, 21.07it/s]\n",
            "jinja2-3.1.2         | 99 KB     | : 100% 1.0/1 [00:00<00:00, 15.99it/s]\n",
            "lcms2-2.12           | 443 KB    | : 100% 1.0/1 [00:00<00:00,  8.91it/s]\n",
            "fftw-3.3.10          | 6.4 MB    | : 100% 1.0/1 [00:01<00:00,  1.24s/it]               \n",
            "bokeh-2.4.3          | 13.3 MB   | : 100% 1.0/1 [00:02<00:00,  2.92s/it]\n",
            "certifi-2022.9.24    | 155 KB    | : 100% 1.0/1 [00:00<00:00, 19.92it/s]\n",
            "zict-2.2.0           | 20 KB     | : 100% 1.0/1 [00:00<00:00, 27.42it/s]\n",
            "mkl-service-2.4.0    | 61 KB     | : 100% 1.0/1 [00:00<00:00, 24.27it/s]\n",
            "pandas-1.5.1         | 11.9 MB   | : 100% 1.0/1 [00:00<00:00,  1.89it/s]               \n",
            "lz4-3.1.3            | 42 KB     | : 100% 1.0/1 [00:00<00:00,  8.59it/s]\n",
            "msgpack-python-1.0.3 | 83 KB     | : 100% 1.0/1 [00:00<00:00, 11.58it/s]\n",
            "partd-1.3.0          | 18 KB     | : 100% 1.0/1 [00:00<00:00, 24.67it/s]\n",
            "mkl_fft-1.3.1        | 213 KB    | : 100% 1.0/1 [00:00<00:00, 13.96it/s]\n",
            "mkl_random-1.2.2     | 379 KB    | : 100% 1.0/1 [00:00<00:00, 10.40it/s]\n",
            "featuretools-1.18.0  | 564 KB    | : 100% 1.0/1 [00:00<00:00,  4.70it/s]\n",
            "python-dateutil-2.8. | 240 KB    | : 100% 1.0/1 [00:00<00:00, 15.03it/s]\n",
            "numexpr-2.8.4        | 133 KB    | : 100% 1.0/1 [00:00<00:00, 12.02it/s]\n",
            "joblib-1.2.0         | 205 KB    | : 100% 1.0/1 [00:00<00:00, 10.73it/s]\n",
            "woodwork-0.20.0      | 206 KB    | : 100% 1.0/1 [00:00<00:00, 10.22it/s]\n",
            "pyparsing-3.0.9      | 79 KB     | : 100% 1.0/1 [00:00<00:00, 20.86it/s]\n",
            "libgfortran-ng-12.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 27.49it/s]\n",
            "libwebp-1.2.4        | 79 KB     | : 100% 1.0/1 [00:00<00:00, 13.34it/s]\n",
            "markupsafe-2.1.1     | 22 KB     | : 100% 1.0/1 [00:00<00:00, 16.83it/s]\n",
            "heapdict-1.0.1       | 7 KB      | : 100% 1.0/1 [00:00<00:00, 24.10it/s]\n",
            "tblib-1.7.0          | 15 KB     | : 100% 1.0/1 [00:00<00:00, 22.53it/s]\n",
            "pyyaml-6.0           | 178 KB    | : 100% 1.0/1 [00:00<00:00, 13.64it/s]\n",
            "six-1.16.0           | 14 KB     | : 100% 1.0/1 [00:00<00:00, 28.94it/s]\n",
            "numpy-base-1.23.4    | 6.7 MB    | : 100% 1.0/1 [00:00<00:00,  3.14it/s]\n",
            "giflib-5.2.1         | 77 KB     | : 100% 1.0/1 [00:00<00:00, 19.91it/s]\n",
            "lerc-3.0             | 216 KB    | : 100% 1.0/1 [00:00<00:00, 14.04it/s]\n",
            "zstd-1.5.2           | 488 KB    | : 100% 1.0/1 [00:00<00:00, 11.56it/s]\n",
            "mkl-2021.4.0         | 142.6 MB  | : 100% 1.0/1 [00:03<00:00,  3.38s/it]               \n",
            "libpng-1.6.37        | 278 KB    | : 100% 1.0/1 [00:00<00:00,  3.77it/s]\n",
            "ca-certificates-2022 | 150 KB    | : 100% 1.0/1 [00:00<00:00, 13.82it/s]\n",
            "tornado-6.1          | 649 KB    | : 100% 1.0/1 [00:00<00:00,  5.70it/s]\n",
            "scikit-learn-1.1.3   | 7.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.17it/s]\n",
            "lz4-c-1.9.3          | 179 KB    | : 100% 1.0/1 [00:00<00:00, 14.45it/s]\n",
            "cloudpickle-2.2.0    | 25 KB     | : 100% 1.0/1 [00:00<00:00, 21.19it/s]\n",
            "korean_lunar_calenda | 12 KB     | : 100% 1.0/1 [00:00<00:00, 24.56it/s]\n",
            "locket-1.0.0         | 8 KB      | : 100% 1.0/1 [00:00<00:00, 27.53it/s]\n",
            "pillow-9.2.0         | 670 KB    | : 100% 1.0/1 [00:00<00:00,  9.36it/s]\n",
            "python_abi-3.9       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 29.47it/s]\n",
            "libwebp-base-1.2.4   | 347 KB    | : 100% 1.0/1 [00:00<00:00, 12.69it/s]\n",
            "sortedcontainers-2.4 | 26 KB     | : 100% 1.0/1 [00:00<00:00, 22.73it/s]\n",
            "threadpoolctl-3.1.0  | 18 KB     | : 100% 1.0/1 [00:00<00:00, 22.49it/s]\n",
            "numpy-1.23.4         | 10 KB     | : 100% 1.0/1 [00:00<00:00, 11.65it/s]\n",
            "libtiff-4.4.0        | 526 KB    | : 100% 1.0/1 [00:00<00:00, 12.23it/s]\n",
            "bottleneck-1.3.5     | 115 KB    | : 100% 1.0/1 [00:00<00:00, 11.64it/s]\n",
            "distributed-2022.11. | 711 KB    | : 100% 1.0/1 [00:00<00:00,  6.92it/s]\n",
            "psutil-5.9.0         | 330 KB    | : 100% 1.0/1 [00:00<00:00, 11.48it/s]\n",
            "fsspec-2022.11.0     | 96 KB     | : 100% 1.0/1 [00:00<00:00,  1.35it/s]\n",
            "jpeg-9e              | 268 KB    | : 100% 1.0/1 [00:00<00:00, 12.15it/s]\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \n",
            "\n",
            "    Installed package of scikit-learn can be accelerated using scikit-learn-intelex.\n",
            "    More details are available here: https://intel.github.io/scikit-learn-intelex\n",
            "\n",
            "    For example:\n",
            "\n",
            "        $ conda install scikit-learn-intelex\n",
            "        $ python -m sklearnex my_application.py\n",
            "\n",
            "    \n",
            "\n",
            "\b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Retrieving notices: ...working... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# SD 2.0 Setup\n"
      ],
      "metadata": {
        "id": "GRZv_ytUVx9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements"
      ],
      "metadata": {
        "id": "HWsYDzedWDU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Stability-AI/stablediffusion"
      ],
      "metadata": {
        "id": "13neG5Vynb1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8291c1fe-59a5-4654-96ed-9c3a0ea46295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stablediffusion'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 144 (delta 23), reused 105 (delta 12), pack-reused 24\u001b[K\n",
            "Receiving objects: 100% (144/144), 68.89 MiB | 7.84 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stablediffusion\n",
        "!conda install pytorch==1.12.1 torchvision==0.13.1 -c pytorch -y\n",
        "!pip install transformers==4.19.2 diffusers invisible-watermark\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoJ8bualWeLM",
        "outputId": "d455bb01-49fa-4c01-f3f0-30965ebda610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stablediffusion\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Collecting package metadata (repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch==1.12.1\n",
            "    - torchvision==0.13.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bzip2-1.0.8                |       h7b6447c_0          78 KB\n",
            "    cudatoolkit-11.3.1         |       h2bc3f7f_2       549.3 MB\n",
            "    ffmpeg-4.3                 |       hf484d3e_0         9.9 MB  pytorch\n",
            "    gmp-6.2.1                  |       h295c915_3         544 KB\n",
            "    gnutls-3.6.15              |       he1e5248_0         1.0 MB\n",
            "    lame-3.100                 |       h7b6447c_0         323 KB\n",
            "    libiconv-1.16              |       h7f8727e_2         736 KB\n",
            "    libidn2-2.3.2              |       h7f8727e_0          81 KB\n",
            "    libtasn1-4.16.0            |       h27cfd23_0          58 KB\n",
            "    libunistring-0.9.10        |       h27cfd23_0         536 KB\n",
            "    nettle-3.7.3               |       hbbd107a_1         809 KB\n",
            "    openh264-2.1.1             |       h4ff587b_0         711 KB\n",
            "    pytorch-1.12.1             |py3.9_cuda11.3_cudnn8.3.2_0        1.19 GB  pytorch\n",
            "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
            "    torchvision-0.13.1         |       py39_cu113         7.5 MB  pytorch\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        1.75 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0 None\n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2 None\n",
            "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0 None\n",
            "  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3 None\n",
            "  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0 None\n",
            "  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0 None\n",
            "  libiconv           pkgs/main/linux-64::libiconv-1.16-h7f8727e_2 None\n",
            "  libidn2            pkgs/main/linux-64::libidn2-2.3.2-h7f8727e_0 None\n",
            "  libtasn1           pkgs/main/linux-64::libtasn1-4.16.0-h27cfd23_0 None\n",
            "  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0 None\n",
            "  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1 None\n",
            "  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0 None\n",
            "  pytorch            pytorch/linux-64::pytorch-1.12.1-py3.9_cuda11.3_cudnn8.3.2_0 None\n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda None\n",
            "  torchvision        pytorch/linux-64::torchvision-0.13.1-py39_cu113 None\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2022.9.2~ --> pkgs/main::ca-certificates-2022.10.11-h06a4308_0 None\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge/noarch::certifi-2022.9.24~ --> pkgs/main/linux-64::certifi-2022.9.24-py39h06a4308_0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "gmp-6.2.1            | 544 KB    | : 100% 1.0/1 [00:00<00:00,  6.31it/s]               \n",
            "pytorch-1.12.1       | 1.19 GB   | : 100% 1.0/1 [02:46<00:00, 166.70s/it]              \n",
            "libidn2-2.3.2        | 81 KB     | : 100% 1.0/1 [00:00<00:00, 14.55it/s]\n",
            "libtasn1-4.16.0      | 58 KB     | : 100% 1.0/1 [00:00<00:00, 16.25it/s]\n",
            "cudatoolkit-11.3.1   | 549.3 MB  | : 100% 1.0/1 [00:09<00:00,  9.26s/it]               \n",
            "libunistring-0.9.10  | 536 KB    | : 100% 1.0/1 [00:00<00:00, 13.21it/s]\n",
            "bzip2-1.0.8          | 78 KB     | : 100% 1.0/1 [00:00<00:00, 11.05it/s]\n",
            "lame-3.100           | 323 KB    | : 100% 1.0/1 [00:00<00:00, 13.76it/s]\n",
            "nettle-3.7.3         | 809 KB    | : 100% 1.0/1 [00:00<00:00, 11.80it/s]\n",
            "libiconv-1.16        | 736 KB    | : 100% 1.0/1 [00:00<00:00, 12.40it/s]\n",
            "ffmpeg-4.3           | 9.9 MB    | : 100% 1.0/1 [00:03<00:00,  3.18s/it]               \n",
            "openh264-2.1.1       | 711 KB    | : 100% 1.0/1 [00:00<00:00, 12.58it/s]\n",
            "pytorch-mutex-1.0    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 22.77it/s]\n",
            "gnutls-3.6.15        | 1.0 MB    | : 100% 1.0/1 [00:00<00:00, 11.79it/s]\n",
            "torchvision-0.13.1   | 7.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.03s/it]               \n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Retrieving notices: ...working... done\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.19.2\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers\n",
            "  Downloading diffusers-0.8.1-py3-none-any.whl (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting invisible-watermark\n",
            "  Downloading invisible_watermark-0.1.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers==4.19.2) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.0/770.0 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers==4.19.2) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers==4.19.2) (2.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.19.2) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.19.2) (1.23.4)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/site-packages (from diffusers) (9.2.0)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.13.1-cp39-cp39-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python>=4.1.0.25\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/site-packages (from invisible-watermark) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.19.2) (3.0.9)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.10.0-py3-none-any.whl (6.2 kB)\n",
            "Collecting protobuf<=3.20.1,>=3.12.2\n",
            "  Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers\n",
            "  Downloading flatbuffers-22.11.23-py2.py3-none-any.whl (26 kB)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.19.2) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.19.2) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.19.2) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.19.2) (2022.9.24)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath>=0.19\n",
            "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, mpmath, flatbuffers, zipp, sympy, regex, PyWavelets, protobuf, opencv-python, humanfriendly, filelock, onnx, importlib-metadata, huggingface-hub, coloredlogs, transformers, onnxruntime, diffusers, invisible-watermark\n",
            "Successfully installed PyWavelets-1.4.1 coloredlogs-15.0.1 diffusers-0.8.1 filelock-3.8.0 flatbuffers-22.11.23 huggingface-hub-0.11.0 humanfriendly-10.0 importlib-metadata-5.1.0 invisible-watermark-0.1.5 mpmath-1.2.1 onnx-1.12.0 onnxruntime-1.13.1 opencv-python-4.6.0.66 protobuf-3.20.1 regex-2022.10.31 sympy-1.11.1 tokenizers-0.12.1 transformers-4.19.2 zipp-3.10.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/stablediffusion\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/site-packages (from stable-diffusion==0.0.1) (1.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from stable-diffusion==0.0.1) (1.23.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from stable-diffusion==0.0.1) (4.64.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/site-packages (from torch->stable-diffusion==0.0.1) (4.4.0)\n",
            "Installing collected packages: stable-diffusion\n",
            "  Running setup.py develop for stable-diffusion\n",
            "Successfully installed stable-diffusion-0.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_HOME=/usr/local/cuda-11.4\n",
        "!conda install -c nvidia/label/cuda-11.4.0 cuda-nvcc -y\n",
        "!conda install -c conda-forge gcc -y\n",
        "!conda install -c conda-forge gxx_linux-64=9.5.0 -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBnfK0ipWsG_",
        "outputId": "3229029b-aaaa-40fc-faac-70334e154985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cuda-nvcc\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cuda-nvcc-11.4.48          |                0        54.6 MB  nvidia/label/cuda-11.4.0\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        54.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cuda-nvcc          nvidia/label/cuda-11.4.0/linux-64::cuda-nvcc-11.4.48-0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "cuda-nvcc-11.4.48    | 54.6 MB   | : 100% 1.0/1 [00:08<00:00,  8.83s/it]\n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\bdone\n",
            "Executing transaction: | \b\bdone\n",
            "Retrieving notices: ...working... done\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - gcc\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    binutils_impl_linux-64-2.39|       he00db2b_1         4.7 MB  conda-forge\n",
            "    gcc-12.2.0                 |      h26027b1_11          24 KB  conda-forge\n",
            "    gcc_impl_linux-64-12.2.0   |      hcc96c02_19        56.3 MB  conda-forge\n",
            "    kernel-headers_linux-64-2.6.32|      he073ed8_15         707 KB  conda-forge\n",
            "    ld_impl_linux-64-2.39      |       hcc3a1bd_1         675 KB  conda-forge\n",
            "    libgcc-devel_linux-64-12.2.0|      h3b97bd3_19         3.3 MB  conda-forge\n",
            "    libgcc-ng-12.2.0           |      h65d4601_19         931 KB  conda-forge\n",
            "    libgomp-12.2.0             |      h65d4601_19         455 KB  conda-forge\n",
            "    libsanitizer-12.2.0        |      h46fd767_19         6.2 MB  conda-forge\n",
            "    libstdcxx-ng-12.2.0        |      h46fd767_19         4.3 MB  conda-forge\n",
            "    openssl-1.1.1s             |       h166bdaf_0         2.1 MB  conda-forge\n",
            "    sysroot_linux-64-2.12      |      he073ed8_15        31.4 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       111.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  binutils_impl_lin~ conda-forge/linux-64::binutils_impl_linux-64-2.39-he00db2b_1 None\n",
            "  gcc                conda-forge/linux-64::gcc-12.2.0-h26027b1_11 None\n",
            "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-12.2.0-hcc96c02_19 None\n",
            "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-2.6.32-he073ed8_15 None\n",
            "  libgcc-devel_linu~ conda-forge/linux-64::libgcc-devel_linux-64-12.2.0-h3b97bd3_19 None\n",
            "  libsanitizer       conda-forge/linux-64::libsanitizer-12.2.0-h46fd767_19 None\n",
            "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.12-he073ed8_15 None\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ld_impl_linux-64   pkgs/main::ld_impl_linux-64-2.38-h118~ --> conda-forge::ld_impl_linux-64-2.39-hcc3a1bd_1 None\n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-12.2.0-h65d4601_19 None\n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-12.2.0-h65d4601_19 None\n",
            "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-12.2.0-h46fd767_19 None\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge None\n",
            "  _openmp_mutex          pkgs/main::_openmp_mutex-5.1-1_gnu --> conda-forge::_openmp_mutex-4.5-2_gnu None\n",
            "  ca-certificates    pkgs/main::ca-certificates-2022.10.11~ --> conda-forge::ca-certificates-2022.9.24-ha878542_0 None\n",
            "  certifi            pkgs/main/linux-64::certifi-2022.9.24~ --> conda-forge/noarch::certifi-2022.9.24-pyhd8ed1ab_0 None\n",
            "  openssl              pkgs/main::openssl-1.1.1s-h7f8727e_0 --> conda-forge::openssl-1.1.1s-h166bdaf_0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "sysroot_linux-64-2.1 | 31.4 MB   | : 100% 1.0/1 [00:04<00:00,  4.87s/it]\n",
            "openssl-1.1.1s       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.91it/s]\n",
            "gcc-12.2.0           | 24 KB     | : 100% 1.0/1 [00:00<00:00, 21.14it/s]\n",
            "binutils_impl_linux- | 4.7 MB    | : 100% 1.0/1 [00:00<00:00,  4.48it/s]\n",
            "_openmp_mutex-4.5    | 23 KB     | : 100% 1.0/1 [00:00<00:00, 14.30it/s]\n",
            "_libgcc_mutex-0.1    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 28.18it/s]\n",
            "libgcc-devel_linux-6 | 3.3 MB    | : 100% 1.0/1 [00:00<00:00,  2.51it/s]\n",
            "gcc_impl_linux-64-12 | 56.3 MB   | : 100% 1.0/1 [00:08<00:00,  8.58s/it]              \n",
            "ld_impl_linux-64-2.3 | 675 KB    | : 100% 1.0/1 [00:00<00:00, 11.19it/s]\n",
            "libgomp-12.2.0       | 455 KB    | : 100% 1.0/1 [00:00<00:00, 10.19it/s]\n",
            "libgcc-ng-12.2.0     | 931 KB    | : 100% 1.0/1 [00:00<00:00,  6.18it/s]\n",
            "kernel-headers_linux | 707 KB    | : 100% 1.0/1 [00:00<00:00,  4.73it/s]\n",
            "libsanitizer-12.2.0  | 6.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.13it/s]\n",
            "libstdcxx-ng-12.2.0  | 4.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.58it/s]\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Retrieving notices: ...working... done\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - gxx_linux-64=9.5.0\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    binutils_linux-64-2.39     |      h5fc0e48_11          24 KB  conda-forge\n",
            "    gcc-9.5.0                  |      h1fea6ba_11          24 KB  conda-forge\n",
            "    gcc_impl_linux-64-9.5.0    |      h99780fb_19        43.5 MB  conda-forge\n",
            "    gcc_linux-64-9.5.0         |      h4258300_11          25 KB  conda-forge\n",
            "    gxx_impl_linux-64-9.5.0    |      h99780fb_19        10.6 MB  conda-forge\n",
            "    gxx_linux-64-9.5.0         |      h43f449f_11          25 KB  conda-forge\n",
            "    libgcc-devel_linux-64-9.5.0|      h0a57e50_19         4.0 MB  conda-forge\n",
            "    libsanitizer-9.5.0         |      h2f262e1_19         6.9 MB  conda-forge\n",
            "    libstdcxx-devel_linux-64-9.5.0|      h0a57e50_19         9.9 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        75.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  binutils_linux-64  conda-forge/linux-64::binutils_linux-64-2.39-h5fc0e48_11 None\n",
            "  gcc_linux-64       conda-forge/linux-64::gcc_linux-64-9.5.0-h4258300_11 None\n",
            "  gxx_impl_linux-64  conda-forge/linux-64::gxx_impl_linux-64-9.5.0-h99780fb_19 None\n",
            "  gxx_linux-64       conda-forge/linux-64::gxx_linux-64-9.5.0-h43f449f_11 None\n",
            "  libstdcxx-devel_l~ conda-forge/linux-64::libstdcxx-devel_linux-64-9.5.0-h0a57e50_19 None\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  gcc                                    12.2.0-h26027b1_11 --> 9.5.0-h1fea6ba_11 None\n",
            "  gcc_impl_linux-64                      12.2.0-hcc96c02_19 --> 9.5.0-h99780fb_19 None\n",
            "  libgcc-devel_linu~                     12.2.0-h3b97bd3_19 --> 9.5.0-h0a57e50_19 None\n",
            "  libsanitizer                           12.2.0-h46fd767_19 --> 9.5.0-h2f262e1_19 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libgcc-devel_linux-6 | 4.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.52it/s]\n",
            "binutils_linux-64-2. | 24 KB     | : 100% 1.0/1 [00:00<00:00, 20.94it/s]\n",
            "gxx_impl_linux-64-9. | 10.6 MB   | : 100% 1.0/1 [00:01<00:00,  1.57s/it]               \n",
            "gcc_impl_linux-64-9. | 43.5 MB   | : 100% 1.0/1 [00:06<00:00,  6.54s/it]               \n",
            "libsanitizer-9.5.0   | 6.9 MB    | : 100% 1.0/1 [00:01<00:00,  1.01s/it]\n",
            "libstdcxx-devel_linu | 9.9 MB    | : 100% 1.0/1 [00:01<00:00,  1.55s/it]\n",
            "gxx_linux-64-9.5.0   | 25 KB     | : 100% 1.0/1 [00:00<00:00, 20.35it/s]\n",
            "gcc-9.5.0            | 24 KB     | : 100% 1.0/1 [00:00<00:00, 17.85it/s]\n",
            "gcc_linux-64-9.5.0   | 25 KB     | : 100% 1.0/1 [00:00<00:00, 19.64it/s]\n",
            "Preparing transaction: / \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\bdone\n",
            "Retrieving notices: ...working... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/xformers.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOXi_vkPjlaX",
        "outputId": "ac3f008a-56e7-416a-d47d-10423aac72a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'xformers'...\n",
            "remote: Enumerating objects: 8668, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 8668 (delta 12), reused 7 (delta 1), pack-reused 8639\u001b[K\n",
            "Receiving objects: 100% (8668/8668), 35.40 MiB | 43.42 MiB/s, done.\n",
            "Resolving deltas: 100% (5847/5847), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/xformers\n",
        "!git submodule update --init --recursive\n",
        "!pip install -r requirements.txt\n",
        "!pip install -ve .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlGcf6EHhloX",
        "outputId": "46f26be2-42f2-4d21-d5dc-65a1e86aa93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/xformers\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.23.4)\n",
            "Requirement already satisfied: pyre-extensions==0.0.23 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.0.23)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from pyre-extensions==0.0.23->-r requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.9/site-packages (from pyre-extensions==0.0.23->-r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.23->-r requirements.txt (line 5)) (0.4.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mUsing pip 22.2.2 from /usr/local/lib/python3.9/site-packages/pip (python 3.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/xformers\n",
            "  Running command python setup.py egg_info\n",
            "  /usr/local/lib/python3.9/site-packages/setuptools/dist.py:530: UserWarning: Normalizing '0.0.15.dev+1b1fd8a.d20221124' to '0.0.15.dev0+1b1fd8a.d20221124'\n",
            "    warnings.warn(tmpl.format(**locals()))\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.9/site-packages/torch/utils/cpp_extension.py:411: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-umyy78pt/xformers.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.9/site-packages (from xformers==0.0.15.dev0+1b1fd8a.d20221124) (1.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from xformers==0.0.15.dev0+1b1fd8a.d20221124) (1.23.4)\n",
            "Requirement already satisfied: pyre-extensions==0.0.23 in /usr/local/lib/python3.9/site-packages (from xformers==0.0.15.dev0+1b1fd8a.d20221124) (0.0.23)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.9/site-packages (from pyre-extensions==0.0.23->xformers==0.0.15.dev0+1b1fd8a.d20221124) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from pyre-extensions==0.0.23->xformers==0.0.15.dev0+1b1fd8a.d20221124) (4.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.23->xformers==0.0.15.dev0+1b1fd8a.d20221124) (0.4.3)\n",
            "Installing collected packages: xformers\n",
            "  Running setup.py develop for xformers\n",
            "    Running command python setup.py develop\n",
            "    /usr/local/lib/python3.9/site-packages/setuptools/dist.py:530: UserWarning: Normalizing '0.0.15.dev+1b1fd8a.d20221124' to '0.0.15.dev0+1b1fd8a.d20221124'\n",
            "      warnings.warn(tmpl.format(**locals()))\n",
            "    running develop\n",
            "    /usr/local/lib/python3.9/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
            "      warnings.warn(\n",
            "    /usr/local/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "      warnings.warn(\n",
            "    running egg_info\n",
            "    writing xformers.egg-info/PKG-INFO\n",
            "    writing dependency_links to xformers.egg-info/dependency_links.txt\n",
            "    writing requirements to xformers.egg-info/requires.txt\n",
            "    writing top-level names to xformers.egg-info/top_level.txt\n",
            "    /usr/local/lib/python3.9/site-packages/torch/utils/cpp_extension.py:411: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    reading manifest file 'xformers.egg-info/SOURCES.txt'\n",
            "    reading manifest template 'MANIFEST.in'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'xformers.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.9/site-packages/torch/utils/cpp_extension.py:813: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.3). Most likely this shouldn't be a problem.\n",
            "      warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "    building 'xformers._C' extension\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/attention.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/attention.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/autograd/matmul.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/autograd/matmul.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cpu/attention.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/attention.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.9/site-packages/torch/include/ATen/cpu/vec/vec256/vec256.h:8,\n",
            "                     from /usr/local/lib/python3.9/site-packages/torch/include/ATen/cpu/vec/vec.h:4,\n",
            "                     from /usr/local/lib/python3.9/site-packages/torch/include/ATen/cpu/vec/functional_base.h:6,\n",
            "                     from /usr/local/lib/python3.9/site-packages/torch/include/ATen/cpu/vec/functional.h:3,\n",
            "                     from /content/xformers/xformers/components/attention/csrc/cpu/attention.cpp:7:\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/ATen/cpu/vec/vec_base.h:974: warning: ignoring #pragma unroll  [-Wunknown-pragmas]\n",
            "      974 | # pragma unroll\n",
            "          |\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cpu/matmul.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/matmul.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cpu/sddmm.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/sddmm.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cpu/sparse_softmax.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/sparse_softmax.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cpu/spmm.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/spmm.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/matmul.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/matmul.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem\n",
            "    ptxas info    : Compiling entry function '_ZN71_GLOBAL__N__41_tmpxft_00003c0d_00000000_7_matmul_cpp1_ii_36aa4f9f_1538830matmul_with_sparse_mask_kernelIN3c104HalfEEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS3_16DefaultPtrTraitsElEENS4_IS5_Lm3ES6_lEES8_NS4_IlLm2ES6_lEE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN71_GLOBAL__N__41_tmpxft_00003c0d_00000000_7_matmul_cpp1_ii_36aa4f9f_1538830matmul_with_sparse_mask_kernelIN3c104HalfEEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS3_16DefaultPtrTraitsElEENS4_IS5_Lm3ES6_lEES8_NS4_IlLm2ES6_lEE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 32 registers, 528 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN71_GLOBAL__N__41_tmpxft_00003c0d_00000000_7_matmul_cpp1_ii_36aa4f9f_1538830matmul_with_sparse_mask_kernelIfEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN71_GLOBAL__N__41_tmpxft_00003c0d_00000000_7_matmul_cpp1_ii_36aa4f9f_1538830matmul_with_sparse_mask_kernelIfEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 32 registers, 528 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN71_GLOBAL__N__41_tmpxft_00003c0d_00000000_7_matmul_cpp1_ii_36aa4f9f_1538830matmul_with_sparse_mask_kernelIdEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN71_GLOBAL__N__41_tmpxft_00003c0d_00000000_7_matmul_cpp1_ii_36aa4f9f_1538830matmul_with_sparse_mask_kernelIdEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 32 registers, 528 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 218049 bytes gmem, 72 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIffLi4ELi8ELi32ELi32ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEES5_S5_S5_S5_S5_S5_S5_NS2_IS3_Lm2ES4_lEES5_S3_NS1_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIffLi4ELi8ELi32ELi32ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEES5_S5_S5_S5_S5_S5_S5_NS2_IS3_Lm2ES4_lEES5_S3_NS1_15PhiloxCudaStateE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 4224 bytes smem, 928 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIffLi4ELi8ELi32ELi32ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEES5_S5_S5_S5_S5_S5_S5_NS2_IS3_Lm2ES4_lEES5_S3_NS1_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIffLi4ELi8ELi32ELi32ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEES5_S5_S5_S5_S5_S5_S5_NS2_IS3_Lm2ES4_lEES5_S3_NS1_15PhiloxCudaStateE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 166 registers, 4224 bytes smem, 928 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float2Li4ELi8ELi32ELi32ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float2Li4ELi8ELi32ELi32ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE\n",
            "        16 bytes stack frame, 12 bytes spill stores, 12 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 4224 bytes smem, 928 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float2Li4ELi8ELi32ELi32ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float2Li4ELi8ELi32ELi32ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 202 registers, 4224 bytes smem, 928 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float4Li4ELi8ELi32ELi32ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float4Li4ELi8ELi32ELi32ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE\n",
            "        64 bytes stack frame, 60 bytes spill stores, 60 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 4224 bytes smem, 928 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float4Li4ELi8ELi32ELi32ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541825attention_backward_kernelIf6float4Li4ELi8ELi32ELi32ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEES6_S6_S6_S6_S6_S6_S6_NS3_IS4_Lm2ES5_lEES6_S4_NS2_15PhiloxCudaStateE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 232 registers, 4224 bytes smem, 928 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIffLi32ELi2ELi4ELi8ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm2ES4_lEES5_S5_S5_S5_S3_NS1_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIffLi32ELi2ELi4ELi8ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm2ES4_lEES5_S5_S5_S5_S3_NS1_15PhiloxCudaStateE\n",
            "        64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 704 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float2Li32ELi2ELi4ELi8ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float2Li32ELi2ELi4ELi8ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE\n",
            "        128 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 165 registers, 704 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float4Li32ELi2ELi4ELi8ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float4Li32ELi2ELi4ELi8ELb0EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE\n",
            "        256 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 704 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIffLi32ELi2ELi4ELi8ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm2ES4_lEES5_S5_S5_S5_S3_NS1_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIffLi32ELi2ELi4ELi8ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm2ES4_lEES5_S5_S5_S5_S3_NS1_15PhiloxCudaStateE\n",
            "        64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 127 registers, 704 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float2Li32ELi2ELi4ELi8ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float2Li32ELi2ELi4ELi8ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE\n",
            "        128 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 130 registers, 704 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float4Li32ELi2ELi4ELi8ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541816attention_kernelIf6float4Li32ELi2ELi4ELi8ELb1EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS2_16DefaultPtrTraitsElEENS3_IS4_Lm2ES5_lEES6_S6_S6_S6_S4_NS2_15PhiloxCudaStateE\n",
            "        256 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 138 registers, 704 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541814dropout_kernelIffLi32ELi2ELi4EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEES3_NS1_15PhiloxCudaStateE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN74_GLOBAL__N__44_tmpxft_00003c2b_00000000_7_attention_cpp1_ii_dc54461e_1541814dropout_kernelIffLi32ELi2ELi4EEEvN2at27GenericPackedTensorAccessorIT_Lm3ENS1_16DefaultPtrTraitsElEES3_NS1_15PhiloxCudaStateE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 116 registers, 440 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention_backward_generic.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention_backward_generic.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem, 32 bytes cmem[3]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention_forward_generic.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention_forward_generic.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem, 32 bytes cmem[3]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 210 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 208 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 237 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE\n",
            "        120 bytes stack frame, 144 bytes spill stores, 200 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 238 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k64.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k64.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::bfloat16_t, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k64.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE\n",
            "        152 bytes stack frame, 168 bytes spill stores, 220 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_10bfloat16_tELb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 216 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 214 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb1ELi128EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb1ELi64EEEvNT_6ParamsE\n",
            "        144 bytes stack frame, 176 bytes spill stores, 248 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb0ELi128EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 239 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k64.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k64.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpA=cutlass::arch::CacheOperation::Always, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, CacheOpB=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=cutlass::half_t, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k64.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80ENS1_6half_tELb0ELi64EEEvNT_6ParamsE\n",
            "        176 bytes stack frame, 192 bytes spill stores, 252 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75ENS1_6half_tELb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70ENS1_6half_tELb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50ENS1_6half_tELb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb0ELi2147483647EEEvNT_6ParamsE\n",
            "        72 bytes stack frame, 64 bytes spill stores, 72 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb0ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb0ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=2147483647, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=2147483647]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb1ELi2147483647EEEvNT_6ParamsE\n",
            "        72 bytes stack frame, 64 bytes spill stores, 72 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb1ELi2147483647EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb1ELi2147483647EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb1ELi128EEEvNT_6ParamsE\n",
            "        72 bytes stack frame, 64 bytes spill stores, 72 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb1ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb1ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=true, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb1ELi64EEEvNT_6ParamsE\n",
            "        32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb1ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb1ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 128, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1038): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<128, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=3, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=128, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1039): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<128, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<128, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 256, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(1162): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=128]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb0ELi128EEEvNT_6ParamsE\n",
            "        72 bytes stack frame, 64 bytes spill stores, 72 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb0ELi128EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb0ELi128EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k64.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k64.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(248): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_prologue_done(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(904): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../gemm/custom_mma_multistage.h(253): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::CustomMmaMultistage<Shape_, IteratorA_, SmemIteratorA_, CacheOpA, IteratorB_, SmemIteratorB_, CacheOpB, ElementC_, LayoutC_, Policy_, Stages, SharedMemoryClear, kMaxK, Enable>::set_zero_outside_bounds(__nv_bool) [with Shape_=cutlass::gemm::GemmShape<64, 64, 32>, IteratorA_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorA_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<64, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpA=cutlass::arch::CacheOperation::Global, IteratorB_=cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 64>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages=2, SharedMemoryClear=cutlass::gemm::SharedMemoryClearOption::kNone, kMaxK=64, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(905): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k64.cu(7): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(989): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::processBlockIJ<skipBoundsChecks>(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::SharedStorage &, AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::OutputFragments &, const AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &, int32_t, int32_t) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=64, skipBoundsChecks=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_backward.h(740): here\n",
            "                instantiation of \"void AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::kernel(AttentionBackwardKernel<ArchTag_, scalar_t_, kIsAligned_, kMaxK>::Params &) [with ArchTag_=cutlass::arch::Sm80, scalar_t_=float, kIsAligned_=false, kMaxK=64]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k64.cu(7): here\n",
            "\n",
            "    ptxas info    : 130 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm80EfLb0ELi64EEEvNT_6ParamsE\n",
            "        32 bytes stack frame, 28 bytes spill stores, 28 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm75EfLb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm70EfLb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb0ELi64EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z33attention_kernel_backward_batchedI23AttentionBackwardKernelIN7cutlass4arch4Sm50EfLb0ELi64EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 608 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.cu(58): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.cu(58): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.cu(64): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.cu(70): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.cu(70): here\n",
            "\n",
            "    ptxas info    : 59 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 126 registers, 496 bytes cmem[0], 16 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu(58): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu(58): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu(64): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::bfloat16_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::bfloat16_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::bfloat16_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu(70): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::bfloat16_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu(70): here\n",
            "\n",
            "    ptxas info    : 59 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 122 registers, 496 bytes cmem[0], 16 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm80ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm75ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm70ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass10bfloat16_tENS1_4arch4Sm50ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.cu(43): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.cu(43): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.cu(49): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 4, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Always, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.cu(55): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.cu(55): here\n",
            "\n",
            "    ptxas info    : 59 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 126 registers, 496 bytes cmem[0], 16 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu(28): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu(28): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu(29): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu(35): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu(35): here\n",
            "\n",
            "    ptxas info    : 59 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 122 registers, 496 bytes cmem[0], 16 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 4 bytes spill stores, 4 bytes spill loads\n",
            "    ptxas info    : Used 128 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm75ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm70ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm50ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.cu(13): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.cu(13): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.cu(14): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.cu(15): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=false, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.cu(15): here\n",
            "\n",
            "    ptxas info    : 59 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        112 bytes stack frame, 104 bytes spill stores, 120 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        152 bytes stack frame, 160 bytes spill stores, 220 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        144 bytes stack frame, 136 bytes spill stores, 184 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb0ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb0ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb0ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb0ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb0ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb0ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu(13): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu(13): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu(14): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "              detected during:\n",
            "                instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, float, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, float, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, cutlass::Array<float, 4, true>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 4>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOpFastF32<cutlass::gemm::GemmShape<32, 32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<32, 32>, float, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<32, 32>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::tfloat32_t, cutlass::layout::RowMajor, cutlass::tfloat32_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(672): here\n",
            "                instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu(15): here\n",
            "\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/../kernel_forward.h(451): warning: variable \"si\" was declared but never referenced\n",
            "              detected during instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=float, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\"\n",
            "    /content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu(15): here\n",
            "\n",
            "    ptxas info    : 59 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        112 bytes stack frame, 104 bytes spill stores, 120 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        152 bytes stack frame, 160 bytes spill stores, 220 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm80ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        144 bytes stack frame, 136 bytes spill stores, 184 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm75ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm70ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb1ELi64ELi64ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb1ELi64ELi64ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb1ELi32ELi128ELb0EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb1ELi32ELi128ELb0EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb1ELi32ELi128ELb1EEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z24attention_kernel_batchedI15AttentionKernelIfN7cutlass4arch4Sm50ELb1ELi32ELi128ELb1EEEvNT_6ParamsE\n",
            "        8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 24 registers, 496 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/sddmm.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/sddmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2IfLi1ELi32ELi32ELi32ELi1EEEviiiPKiS3_S3_PKfS5_Pfi' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2IfLi1ELi32ELi32ELi32ELi1EEEviiiPKiS3_S3_PKfS5_Pfi\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 72 registers, 128 bytes smem, 420 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2I6float2Li2ELi32ELi32ELi16ELi1EEEviiiPKiS4_S4_PKfS6_Pfi' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2I6float2Li2ELi32ELi32ELi16ELi1EEEviiiPKiS4_S4_PKfS6_Pfi\n",
            "        8 bytes stack frame, 8 bytes spill stores, 8 bytes spill loads\n",
            "    ptxas info    : Used 80 registers, 256 bytes smem, 420 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi1EEEviiiPKiS4_S4_PKfS6_Pfi' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi1EEEviiiPKiS4_S4_PKfS6_Pfi\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 81 registers, 512 bytes smem, 420 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi0EEEviiiPKiS4_S4_PKfS6_Pfi' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik64_GLOBAL__N__40_tmpxft_0000412c_00000000_7_sddmm_cpp1_ii_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi0EEEviiiPKiS4_S4_PKfS6_Pfi\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 70 registers, 512 bytes smem, 420 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/sddmm2_cuda.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/sddmm2_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem\n",
            "    ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCSR1ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7ge_spmm14sddmmCSR1ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 48 registers, 416 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCSR2ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7ge_spmm14sddmmCSR2ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 44 registers, 416 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCOO1ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7ge_spmm14sddmmCOO1ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 48 registers, 416 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCOO2ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7ge_spmm14sddmmCOO2ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 44 registers, 416 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCOO4ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7ge_spmm14sddmmCOO4ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 78 registers, 416 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/sparse_softmax.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/sparse_softmax.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik73_GLOBAL__N__49_tmpxft_0000416a_00000000_7_sparse_softmax_cpp1_ii_4ff6710c19SparseSoftmaxKernelEiiPKfPKiS4_S4_Pfi' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik73_GLOBAL__N__49_tmpxft_0000416a_00000000_7_sparse_softmax_cpp1_ii_4ff6710c19SparseSoftmaxKernelEiiPKfPKiS4_S4_Pfi\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 18 registers, 404 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_Z27SparseSoftmaxBackwardKerneliiPKfS0_PKiS2_S2_Pfi' for 'sm_80'\n",
            "    ptxas info    : Function properties for _Z27SparseSoftmaxBackwardKerneliiPKfS0_PKiS2_S2_Pfi\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 20 registers, 412 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/cuda/spmm.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/spmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 64 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 72 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 96 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 114 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 72 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 255 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 68 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 160 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 96 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 114 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 70 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 94 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 66 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 80 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 66 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 66 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 64 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 96 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 64 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 64 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7sputnik63_GLOBAL__N__39_tmpxft_00004188_00000000_7_spmm_cpp1_ii_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 168 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/matmul.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/matmul.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/sddmm.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/sddmm.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/sparse_softmax.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/sparse_softmax.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/attention/csrc/spmm.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/spmm.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/swiglu/cuda/dual_gemm_silu_identity_mul.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/cuda/dual_gemm_silu_identity_mul.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0EEENSO_ISV_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESY_Li16EEELSU_1EfSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SQ_SD_S11_fSF_NS14_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SW_fSF_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1F_Li1EEELi3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1E_Li1ENS1K_22PredicatedTileIteratorINS1K_26OutputTileOptimalThreadMapINS1K_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1O_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1J_4warp24FragmentIteratorTensorOpIS16_S19_fNSL_IfLi4ELb1EEESF_EENS1U_20TileIteratorTensorOpIS16_S19_fSF_EENS1K_18SharedLoadIteratorINS1R_18CompactedThreadMapEfLi32EEENS1J_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi1ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0EEENSO_ISV_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESY_Li16EEELSU_1EfSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SQ_SD_S11_fSF_NS14_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SW_fSF_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1F_Li1EEELi3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1E_Li1ENS1K_22PredicatedTileIteratorINS1K_26OutputTileOptimalThreadMapINS1K_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1O_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1J_4warp24FragmentIteratorTensorOpIS16_S19_fNSL_IfLi4ELb1EEESF_EENS1U_20TileIteratorTensorOpIS16_S19_fSF_EENS1K_18SharedLoadIteratorINS1R_18CompactedThreadMapEfLi32EEENS1J_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi1ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 208 registers, 1064 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0EEENSO_ISV_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESY_Li16EEELSU_1EfSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SQ_SD_S11_fSF_NS14_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SW_fSF_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1F_Li1EEELi3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1E_Li1ENS1K_22PredicatedTileIteratorINS1K_26OutputTileOptimalThreadMapINS1K_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1O_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1J_4warp24FragmentIteratorTensorOpIS16_S19_fNSL_IfLi4ELb1EEESF_EENS1U_25TileIteratorTensorOpMixedIS16_S19_fLi32ELi16ELi8ELi8EEENS1K_23SharedLoadIteratorMixedINS1R_18CompactedThreadMapEfLi32ELi16ELi8ELi8EEENS1J_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi2ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0EEENSO_ISV_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESY_Li16EEELSU_1EfSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SQ_SD_S11_fSF_NS14_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SW_fSF_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1F_Li1EEELi3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1E_Li1ENS1K_22PredicatedTileIteratorINS1K_26OutputTileOptimalThreadMapINS1K_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1O_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1J_4warp24FragmentIteratorTensorOpIS16_S19_fNSL_IfLi4ELb1EEESF_EENS1U_25TileIteratorTensorOpMixedIS16_S19_fLi32ELi16ELi8ELi8EEENS1K_23SharedLoadIteratorMixedINS1R_18CompactedThreadMapEfLi32ELi16ELi8ELi8EEENS1J_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi2ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 208 registers, 1064 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/swiglu/cuda/gemm_fused_operand_sum.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/cuda/gemm_fused_operand_sum.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem, 32 bytes cmem[3]\n",
            "    ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0EEENSO_ISV_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSU_1EfSW_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SQ_SD_SZ_fSW_NS12_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SW_SD_SF_fSW_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1D_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1C_Li1ENS1I_22PredicatedTileIteratorINS1I_26OutputTileOptimalThreadMapINS1I_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1M_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1H_4warp24FragmentIteratorTensorOpIS14_S17_fNSL_IfLi4ELb1EEESW_EENS1S_20TileIteratorTensorOpIS14_S17_fSW_EENS1I_18SharedLoadIteratorINS1P_18CompactedThreadMapEfLi32EEENS1H_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi1ELi1EEENS1I_22EpilogueGemmKReductionIfSD_S7_S1C_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0EEENSO_ISV_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSU_1EfSW_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SQ_SD_SZ_fSW_NS12_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SW_SD_SF_fSW_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1D_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1C_Li1ENS1I_22PredicatedTileIteratorINS1I_26OutputTileOptimalThreadMapINS1I_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1M_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1H_4warp24FragmentIteratorTensorOpIS14_S17_fNSL_IfLi4ELb1EEESW_EENS1S_20TileIteratorTensorOpIS14_S17_fSW_EENS1I_18SharedLoadIteratorINS1P_18CompactedThreadMapEfLi32EEENS1H_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi1ELi1EEENS1I_22EpilogueGemmKReductionIfSD_S7_S1C_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE\n",
            "        304 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 232 registers, 736 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0EEENSO_ISV_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSU_1EfSW_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SQ_SD_SZ_fSW_NS12_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SW_SD_SF_fSW_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1D_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1C_Li1ENS1I_22PredicatedTileIteratorINS1I_26OutputTileOptimalThreadMapINS1I_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1M_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1H_4warp24FragmentIteratorTensorOpIS14_S17_fNSL_IfLi4ELb1EEESW_EENS1S_25TileIteratorTensorOpMixedIS14_S17_fLi32ELi16ELi8ELi8EEENS1I_23SharedLoadIteratorMixedINS1P_18CompactedThreadMapEfLi32ELi16ELi8ELi8EEENS1H_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi2ELi1EEENS1I_22EpilogueGemmKReductionIfSD_S7_S1C_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0EEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0EEENSO_ISV_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSU_1EfSW_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SQ_SD_SZ_fSW_NS12_17MmaTensorOpPolicyINSS_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SW_SD_SF_fSW_NSS_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1D_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1C_Li1ENS1I_22PredicatedTileIteratorINS1I_26OutputTileOptimalThreadMapINS1I_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1M_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ENSE_9NoPermuteELb0EEENS1H_4warp24FragmentIteratorTensorOpIS14_S17_fNSL_IfLi4ELb1EEESW_EENS1S_25TileIteratorTensorOpMixedIS14_S17_fLi32ELi16ELi8ELi8EEENS1I_23SharedLoadIteratorMixedINS1P_18CompactedThreadMapEfLi32ELi16ELi8ELi8EEENS1H_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2EEENSB_ILi0ELi8EEELi2ELi1EEENS1I_22EpilogueGemmKReductionIfSD_S7_S1C_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE\n",
            "        304 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 232 registers, 736 bytes cmem[0]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/swiglu/cuda/silu_bw_fused.cu -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/cuda/silu_bw_fused.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DHAS_PYTORCH --use_fast_math --generate-line-info -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -DNDEBUG --threads 4 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    ptxas info    : 1 bytes gmem\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE6_clEvEUlN3c108BFloat16ESA_SA_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESH_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE6_clEvEUlN3c108BFloat16ESA_SA_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESH_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 30 registers, 1624 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE6_clEvEUlN3c108BFloat16ESA_SA_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESH_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE6_clEvEUlN3c108BFloat16ESA_SA_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESH_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 28 registers, 418 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE4_clEvEUlN3c104HalfESA_SA_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESH_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE4_clEvEUlN3c104HalfESA_SA_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESH_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 28 registers, 1624 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE4_clEvEUlN3c104HalfESA_SA_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESH_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE4_clEvEUlN3c104HalfESA_SA_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESH_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 27 registers, 418 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE2_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESF_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE2_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESF_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 29 registers, 1624 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE2_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESF_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE2_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESF_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 32 registers, 418 bytes cmem[0]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE0_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESF_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE0_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESF_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 44 registers, 1624 bytes cmem[0], 88 bytes cmem[2]\n",
            "    ptxas info    : Compiling entry function '_ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE0_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESF_EEviT0_T1_T2_T3_' for 'sm_80'\n",
            "    ptxas info    : Function properties for _ZN2at6native78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690145unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN78_GLOBAL__N__48_tmpxft_000041f6_00000000_7_silu_bw_fused_cpp1_ii_c4fa0e23_1690113silu_bw_fusedERKNS_6TensorES6_S6_ENKUlvE_clEvENKUlvE0_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESF_EEviT0_T1_T2_T3_\n",
            "        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "    ptxas info    : Used 44 registers, 418 bytes cmem[0], 88 bytes cmem[2]\n",
            "    /usr/local/lib/python3.9/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/swiglu/swiglu_op.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/swiglu_op.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    gcc -pthread -B /usr/local/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /usr/local/include -I/usr/local/include -fPIC -O2 -isystem /usr/local/include -fPIC -I/content/xformers/xformers/components -I/content/xformers/third_party/sputnik -I/content/xformers/third_party/cutlass/include -I/content/xformers/third_party/cutlass/examples -I/usr/local/lib/python3.9/site-packages/torch/include -I/usr/local/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/site-packages/torch/include/TH -I/usr/local/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.9 -c /content/xformers/xformers/components/swiglu/swiglu_packedw.cpp -o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/swiglu_packedw.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    g++ -pthread -B /usr/local/compiler_compat -shared -Wl,-rpath,/usr/local/lib -Wl,-rpath-link,/usr/local/lib -L/usr/local/lib -L/usr/local/lib -Wl,-rpath,/usr/local/lib -Wl,-rpath-link,/usr/local/lib -L/usr/local/lib build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/attention.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/autograd/matmul.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/attention.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/matmul.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/sddmm.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/sparse_softmax.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cpu/spmm.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/matmul.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention_backward_generic.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/attention_forward_generic.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k128.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_bf16_k64.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k128.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f16_k64.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k128.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/backward_f32_k64.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_bf16_aligned.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f16_aligned.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/mem_eff_attention/kernels/forward_f32_aligned.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/sddmm.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/sddmm2_cuda.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/sparse_softmax.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/cuda/spmm.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/matmul.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/sddmm.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/sparse_softmax.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/attention/csrc/spmm.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/cuda/dual_gemm_silu_identity_mul.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/cuda/gemm_fused_operand_sum.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/cuda/silu_bw_fused.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/swiglu_op.o build/temp.linux-x86_64-cpython-39/content/xformers/xformers/components/swiglu/swiglu_packedw.o -L/usr/local/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-cpython-39/xformers/_C.so\n",
            "    copying build/lib.linux-x86_64-cpython-39/xformers/_C_flashattention.so -> xformers\n",
            "    copying build/lib.linux-x86_64-cpython-39/xformers/_C.so -> xformers\n",
            "    Creating /usr/local/lib/python3.9/site-packages/xformers.egg-link (link to .)\n",
            "    Adding xformers 0.0.15.dev0+1b1fd8a.d20221124 to easy-install.pth file\n",
            "\n",
            "    Installed /content/xformers\n",
            "Successfully installed xformers-0.0.15.dev0+1b1fd8a.d20221124\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}